{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff861b42-f9a1-4d2a-af32-bc50edac915a",
   "metadata": {},
   "source": [
    "# Project 5: Semi-supervised image classification\n",
    "\n",
    "### Howard Nguyen-Huu (20825782)\n",
    "\n",
    "> assuming that only M out of N images in the traingin data have ground truth labels, design and implement a weakly supervsied training of classification network that can benefit from unlabeled examples in the training dataset (e.g. MNIST or CIFAR-10, but you need to ignore labels on a subset of training examples).\n",
    "> - Prof. Yuri Boykov\n",
    "\n",
    "### Contributions\n",
    "\n",
    "- single contributor: Howard Nguyen-Huu: research, design, implementation, and experiments \n",
    "\n",
    "## Approach and Implementation\n",
    "\n",
    "- The proposed methodology employs a convolutional neural network (CNN) with an EfficientNet backbone for feature extraction.\n",
    "- The model architecture is designed to facilitate semi-supervised learning by incorporating supervised, clustering, and consistency losses\n",
    "- Clustering is performed using KMeans or Soft KMeans to group feature vectors, while consistency losses enforce invariant predictions under different data augmentations\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- The Imagenette dataset serves as the benchmark for evaluating the model's performance\n",
    "- this dataset is a subset of the larger ImageNet dataset, comprising 10 easily classified classes\n",
    "- I chose it for this project due to its manageable size and diversity, making it suitable for experimentation without the computational overhead associated with the full ImageNet dataset, (since I wanted to train the models from scratch instead of using pretrained CNN weights)\n",
    "- The dataset provides a balanced distribution of classes, which is essential for training robust classification models\n",
    "\n",
    "#### Split of Labeled/Unlabeled data\n",
    "\n",
    "- In my approach a value $p \\in [0, 1]$ determines the proportion of the dataset that is labeled\n",
    "  - ex. $p = 0.1$ indicates that 10% of the data is labeled, while the remaining 90% is unlabeled\n",
    "  - This split is implemented in the `split_labeled_unlabeled` method of the `SemiSupervisedTrainer` class, ensuring a random and unbiased division of the dataset:\n",
    "\n",
    "```python\n",
    "def split_labeled_unlabeled(self, dataset: torch.utils.data.Dataset) -> Tuple[Subset, Subset]:\n",
    "    num_samples = len(dataset)\n",
    "    num_labeled = int(num_samples * self.labeled_ratio)\n",
    "    indices = torch.randperm(num_samples).tolist()\n",
    "    labeled_indices = indices[:num_labeled]\n",
    "    unlabeled_indices = indices[num_labeled:]\n",
    "    return Subset(dataset, labeled_indices), Subset(dataset, unlabeled_indices)\n",
    "```\n",
    "\n",
    "- for the majority of my experiments I ran them for each of $p \\in [0.1, 0.25, 0.5, 0.75]$\n",
    "\n",
    "\n",
    "### Deep Learning Model\n",
    "\n",
    "- model architecture is defined in the `SemiSupervisedClassifier` class within `representations/models/semisupclassifier.py`\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "- I used EfficientNet CNN as the convolutional backbone, because it scored relatively well on Imagenet for the small number of parameters (i trained on a GPU with 24gb vram with a batch size of 32)\n",
    "\n",
    "#### Feature Extraction and Classification\n",
    "\n",
    "- The CNN backbone outputs a feature vector, which is then flattened and connected to a fully connected (Linear) layer with an output dimension of 512\n",
    "- this 512-dimensional vector serves is the clustering feature, which is intended to be clustered during training\n",
    "- Following the clustering feature, two additional fully connected layers with ReLU activations are used as the classifier, the output layer outputs 10 logits for each class in Imagenette\n",
    "- note: the feature vectors are normalized to improve clustering performance\n",
    "  - Normalization ensures that the feature space is isotropic, preventing any single feature from dominating due to scale differences\n",
    "  - This was a very, very crucial step is crucial, and was found when initial experiments without normalization resulted in significantly worse clustering and classification performance\n",
    "  - the normalization is implemented in the `forward` method of the `SemiSupervisedClassifier`:\n",
    "\n",
    "```python\n",
    "def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    features = self.backbone(x)\n",
    "    logits = self.classifier(features)\n",
    "    return F.normalize(features, dim=1), logits\n",
    "```\n",
    "\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "- My loss function combines multiple loss components to guide the model in learning from both labeled and unlabeled data\n",
    "- found in the `SemiSupervisedLoss` class, located in `representations/losses/semisuploss.py`\n",
    "- 3 loss types: supervised, clustering, and consistency losses\n",
    "- The relative importance of each loss component is controlled by lambda parameters, allowing for fine-tuning based on empirical performance\n",
    "\n",
    "#### Supervised Loss\n",
    "\n",
    "- The supervised loss is a standard cross-entropy loss applied to labeled data, encouraging the model to correctly classify samples with known labels:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{supervised}} = -\\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "- where:\n",
    "  - $N$ is the number of labeled samples\n",
    "  - $y_i$ is the true label\n",
    "  - $\\hat{y}_i$ is the predicted probability\n",
    "- implementation:\n",
    "\n",
    "```python\n",
    "self.supervised_loss = nn.CrossEntropyLoss()\n",
    "sup_loss = self.supervised_loss(logits_labeled, labels)\n",
    "```\n",
    "\n",
    "#### Clustering Loss\n",
    "\n",
    "\n",
    "- The clustering loss aims to group feature vectors of unlabeled data into clusters that correspond to the underlying classes\n",
    "- this was intended to force the network to learn of meaningful representations even without explicit labels\n",
    "- I used both:\n",
    "  - **KMeans Clustering:** Assigns each feature vector to the nearest centroid, providing hard assignments.\n",
    "  - **Soft KMeans Clustering:** Offers soft assignments, allowing feature vectors to partially belong to multiple clusters.\n",
    "\n",
    "- The clustering loss is defined as a cross-entropy loss between the model's logits for unlabeled data and their cluster assignments:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{clustering}} = \\frac{1}{M} \\sum_{j=1}^{M} \\text{CE}(\\hat{y}_j, c_j)\n",
    "$$\n",
    "\n",
    "- where:\n",
    "  - $M$ is the number of unlabeled samples\n",
    "  - $\\hat{y}_j$ are the logits\n",
    "  - $c_j$ are the cluster assignments.\n",
    "\n",
    "- Clustering assignments are updated using the `update_clusters` method of the `cluster_assigner` object, which can be either `KMeansClusterAssignment` or `SoftKMeansClusterAssignment` both in `representations/losses/kmeans.py`\n",
    "\n",
    "- The clustering loss computation is as follows:\n",
    "\n",
    "```python\n",
    "self.cluster_assigner = (\n",
    "    SoftKMeansClusterAssignment(num_classes, feature_dim)\n",
    "    if soft_kmeans\n",
    "    else KMeansClusterAssignment(num_classes, feature_dim)\n",
    ")\n",
    "cluster_assignments = self.cluster_assigner.update_clusters(all_features)\n",
    "cluster_loss = F.cross_entropy(logits_unlabeled[:min_batch_size], cluster_assignments[:min_batch_size])\n",
    "```\n",
    "\n",
    "- i experimented with a cosine ramp-up schedule to gradually increases the weight of the clustering loss over the initial epochs to ensure stable training:\n",
    "\n",
    "$$\n",
    "\\text{scale} = \\frac{1 - \\cos\\left(\\frac{\\pi \\times \\text{epoch}}{\\text{cluster\\_rampup\\_epochs}}\\right)}{2}\n",
    "$$\n",
    "\n",
    "#### Consistency Loss(es)\n",
    "\n",
    "- Augmentation Consistency, classifier logits from the same unlabeled samples weak and strong image augmentations softmaxed to get a probability which is then compared to each other using KL divergence\n",
    "- Intuition: we can still make the network learn that it should have similar classifications for the same unlabeled images regardless of the augmentations\n",
    "- Clustering Consistency, uses the same approach and method but with the classifier output logits and the distances from the nearest centroid\n",
    "\n",
    "##### Augmentation Consistency\n",
    "\n",
    "- I used augmentation consistency loss to make the model's predictions remain consistent under different augmentations of the same unlabeled image\n",
    "- This enhances the model's robustness to variations in input data. The loss is computed as the symmetric Kullback-Leibler (KL) divergence between the softmax outputs of weakly and strongly augmented views:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{aug\\_consistency}} = \\frac{1}{2} \\left( \\text{KL}(P_{\\text{weak}} \\| P_{\\text{strong}}) + \\text{KL}(P_{\\text{strong}} \\| P_{\\text{weak}}) \\right)\n",
    "$$\n",
    "\n",
    "- where:\n",
    "  - $P_{\\text{weak}}$ and $P_{\\text{strong}}$ are the probability distributions obtained from weak and strong augmentations, respectively\n",
    "\n",
    "- the augmentations used can be found in the initialization of `representations/datasets/imagenette.py` as `self.weak_augmentations` and `self.strong_augmentations` (using `torchvision` builtin augmentations) including:\n",
    "  - ColorJitter\n",
    "  - RandomAutocontrast\n",
    "  - RandomEqualize\n",
    "  - RandomPosterize\n",
    "  - RandomSolarize\n",
    "  - RandomErasing\n",
    "\n",
    "- my implementation also uses temperature scaling to smooth the probability distributions before computing KL divergence:\n",
    "\n",
    "```python\n",
    "def consistency_loss(self, logits1, logits2):\n",
    "    probs1 = F.softmax(logits1 / self.temperature, dim=-1)\n",
    "    probs2 = F.softmax(logits2 / self.temperature, dim=-1)\n",
    "    return 0.5 * (\n",
    "        F.kl_div(probs1.log(), probs2, reduction=\"batchmean\")\n",
    "        + F.kl_div(probs2.log(), probs1, reduction=\"batchmean\")\n",
    "    )\n",
    "aug_consistency_loss = self.consistency_loss(logits_aug1, logits_aug2)\n",
    "```\n",
    "\n",
    "##### Clustering Consistency\n",
    "\n",
    "- Clustering consistency loss aligns the model's predictions with the cluster assignments derived from feature vectors\n",
    "- This ensures that the classifier's logits correspond to the semantic groupings identified during clustering\n",
    "- The loss is calculated as the average cross-entropy loss between the predicted logits and the nearest cluster assignments for both labeled and unlabeled data:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{cluster\\_consistency}} = \\frac{1}{2} \\left( \\text{CrossEntropy}(\\text{logits}_\\text{unlabeled}, \\text{cluster\\_assignments}_\\text{unlabeled}) + \\text{CrossEntropy}(\\text{logits}_\\text{labeled}, \\text{cluster\\_assignments}_\\text{labeled}) \\right)\n",
    "$$\n",
    "\n",
    "- The implementation involves computing the logits corresponding to cluster assignments and applying cross-entropy loss:\n",
    "\n",
    "```python\n",
    "unlabeled_cluster_logits = self.cluster_assigner.compute_logits(features_unlabeled)\n",
    "labeled_cluster_logits = self.cluster_assigner.compute_logits(features_labeled)\n",
    "cluster_consistency_loss = 0.5 * (\n",
    "    F.cross_entropy(logits_unlabeled, unlabeled_cluster_logits.argmax(dim=1)) +\n",
    "    F.cross_entropy(logits_labeled, labeled_cluster_logits.argmax(dim=1))\n",
    ")\n",
    "```\n",
    "\n",
    "#### Loss Calculations\n",
    "\n",
    "- The total loss is a weighted sum of all individual loss components, controlled by lambda parameters:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{supervised}} + \\lambda_{\\text{cluster}} \\times \\mathcal{L}_{\\text{clustering}} + \\lambda_{\\text{aug\\_consistency}} \\times \\mathcal{L}_{\\text{aug\\_consistency}} + \\lambda_{\\text{cluster\\_consistency}} \\times \\mathcal{L}_{\\text{cluster\\_consistency}}\n",
    "$$\n",
    "\n",
    "- The lambda parameters determine the relative importance of each loss component and are tuned based on empirical performance\n",
    "\n",
    "### Training loop\n",
    "\n",
    "\n",
    "- for each labeled batch:\n",
    "  - sample an unlabeled batch, (looping back to the first batch if it runs out\n",
    "  - apply strong augmentations to the labeled data, apply both weak and strong augmentations to the unlabeled data\n",
    "  - get features and classification logits for labeled data and both weakly and stronly augmented unlabeled data\n",
    "  - compute combined loss is (supervised, clustering, and consistency losses based on lambdas)\n",
    "\n",
    "### Validation\n",
    "\n",
    "- i seperated a validation set which has not been seen at all in the training data to validate\n",
    "- this ensures an unbiased assessment of the model's capabilities\n",
    "- instead of overall accuracy, I used per-class mean accuracy\n",
    "  - in early testing the model accuracy looked higher even when the model did not perform well, due to class imbalances\n",
    "  - this is a better representation of how the model performs across all classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f2b381-025d-405e-a11c-55eecbb02686",
   "metadata": {},
   "source": [
    "## Reproduction of results\n",
    "\n",
    "- results can be modified using the below code blocks (after downloading and extracting imagenette):\n",
    "- however it is recommended to just use the training script found in `scripts/run.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dd5d2aa-aa44-4707-add4-765b753f7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from representations.datasets.imagenette import ImagenetteDataModule\n",
    "from representations.trainers.trainer import SemiSupervisedTrainer\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6061447c-6212-45ee-b183-a03a6d6786f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ImagenetteDataModule(Path(\"/home/howard/representations/data/imagenette2-320/\"))\n",
    "\n",
    "feature_dim = 512\n",
    "loss_args = {\n",
    "    \"lambda_cluster\": 1.0,\n",
    "    \"cluster_all\": True,\n",
    "    \"lambda_aug_consistency\": 1.0,\n",
    "    \"lambda_cluster_consistency\": 0.0,\n",
    "    \"soft_kmeans\": True\n",
    "}\n",
    "labeled_ratio = 0.5 # fraction of dataset that is labeled\n",
    "\n",
    "trainer = SemiSupervisedTrainer(num_classes=10, labeled_ratio=0.5, pretrained=False, loss_args=loss_args)\n",
    "_ = trainer.train(data, num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ae9cf7-a50b-4238-b652-333c8249fae0",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "- The evaluation criteria inlcude soundness of the insights and algorithms, efficiency of your implementation, meaningfulness and comprehensiveness of evaluation, as well as clarity. The methodology should be clearly motivated\n",
    "- The good quality of the results is desirable, but it is not absolutely necessary particularly if you undertake a more complex project\n",
    "- However, the results (good or bad) should make sense. In case of weak or negative results, your conclusions should find a conceptual explanation for the failures of the selected methodology\n",
    "- Discussion of clearly motivated rectification steps should help as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
